{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWjIXT3yzQ+KCBbc4DonkG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sphilipCS/nanogpt/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WavxQKxz6nqi",
        "outputId": "3ef878d9-0423-4d83-b5a1-63aee7767631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-28 00:34:41--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-06-28 00:34:42 (26.8 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt.1', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    print(\"length of dataset in characters: \", len(text))\n",
        "\n",
        "    # if you want to pring the first few charchters\n",
        "    print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojp7inef9WhQ",
        "outputId": "69f65cc5-8ccc-45b3-c303-a5cc2685ea7a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets look at the first 100 charchters\n"
      ],
      "metadata": {
        "id": "_RrdtUf5928t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)\n",
        "# here we use set to get all the charachters from the input.txt and then we put this into a list and we sort this as well.\n",
        "print(chars)\n",
        "print(''.join(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkB1WRNy-ps3",
        "outputId": "f28ab383-1744-4ec6-a0a7-8f8fd989ef25"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we are going to tokenize the input text. These dictionaries are useful for tokenizing text, which means converting text into a sequence of numbers that a machine learning model can understand.\n",
        "\n",
        "# This line creates a dictionary called stoi (string to integer). It iterates through the chars list (which contains all the unique characters in your text). For each character ch and its index i, it creates an entry in the dictionary where the character is the key and the index is the value. So, stoi maps each character to a unique integer.\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "# This line creates a dictionary called itos (integer to string). It does the opposite of stoi. It iterates through the chars list using enumerate to get both the index i and the character ch. It then creates entries in the dictionary where the index is the key and the character is the value. So, itos maps each integer back to its corresponding character.\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b3GcSEU8Ct6M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}